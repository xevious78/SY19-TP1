\documentclass[]{report}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%opening
\title{SY19}
\author{}

\begin{document}
	
\lstset{frame=tb,
	language=R,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\maketitle

\part{Abreviations}
	

\begin{abstract}

\end{abstract}

\part{Introduction}

\part{Ex 1}

\part{Ex 2 - Breast Cancer Recurring Time}

\section{Introduction}
This part aims to build the best model to predict the recurring time of breast cancer based on about 30 features computed from a breast mass.  This regression problem will take advantage of a given dataset describing about 200 patient cases.

\section{Dataset Description}
We take a look at the raw dataset to get first hints on how each feature contributes to the recurring time.

The dataset comprises 194 patient cases, each of which is described through 32 features and the cancer recurring time \texttt{Time} that we have to predict.

\subsection{Time}
Let's first describe the evolution of the variable \texttt{Time}. To do so, we can use the R functions \texttt{boxplot} and \texttt{hist}.

\begin{figure}[!hb]
	\centering
	\input{Figures/time_boxplot.tex}
	\caption{Box Plot}
\end{figure}

\begin{figure}[!hb]
	\centering
	\input{Figures/time_hist.tex}
	\caption{Histogram}
\end{figure}

\subsection{Features Description}
Each patient is represented with a set of 32 features extracted and computed from a digitized image of a breast mass. 

The data description we were given does not specify the units, but we do not need them for the following analysis.

Here are the 32 features we are provided with:  
\begin{itemize}
	\item Lymph Node Status
	
	\item Mean of 
		\begin{itemize}
			\item radius
			\item texture
			\item perimeter
			\item area
			\item smoothness
			\item compactness
			\item concavity
			\item concave points
			\item symmetry
			\item fractal dimension
		\end{itemize}
	
	\item Standard Error of 
		\begin{itemize}
		\item radius
		\item texture
		\item perimeter
		\item area
		\item smoothness
		\item compactness
		\item concavity
		\item concave points
		\item symmetry
		\item fractal dimension
	\end{itemize}

	\item Mean of the three largest values (also called "Worst") of 
		\begin{itemize}
		\item radius
		\item texture
		\item perimeter
		\item area
		\item smoothness
		\item compactness
		\item concavity
		\item concave points
		\item symmetry
		\item fractal dimension
	\end{itemize}

	\item Tumor Size
\end{itemize}

\subsubsection{Feature Correlation}
Based on the definition of the parameters described above, we already know that many features are correlated :
\begin{itemize}
	\item The mean of each parameters is smaller than the "worst" value;
	\item The radius, the perimeter and the area might be linked together;
	\item The compactness can be computed with the perimeter and the area thanks to the given formula : $Compactness = \frac{perimeter^2}{area - 1}$
\end{itemize}

\subsection{Data Relevance}
We should first check that every patient is relevant to our study, in other words, that there is no abnormal observation in the dataset. Cook's Distance is an interesting measure to verify this important criteria, it can be computed after a simple Linear Regression.

Cook's distance aims to study the influence of each observation on the regression coefficient estimates. To do so, this method uses a straight-forward approach that consists in computing the difference between the original coefficient estimates $\hat{\beta}$ and the coefficient estimates without taking into account the $i$th observation $\hat{\beta}_{-i}$. The difference is then normalized using the number of parameters and the standard deviation estimate. 

A value higher than 1 often indicates an outlier that should be removed from the dataset.

In R, we can use the following code to compute and plot the Cook's distance of each observation :

\begin{lstlisting}
linreg = lm(Time ~ ., data=data_set)
cooks.distance(linreg)
\end{lstlisting} 

\begin{figure}[!hb]
	\centering
	\input{Figures/cooks_distance.tex}
	\caption{Cook's Distance}
	\label{fig:cook_distance}
\end{figure}

According to plot (figure \ref{fig:cook_distance}), no observation is located beyond the critical Cook's boundary of 1. This means that we can potentially use each and every patient case of our dataset to build our regression model. 

\subsection{Relation between "feature" and "time"}
In this section, we will take a first look at the relationship between the variable \texttt{Time} and the features used to describe a patient case. 

A first way to do it is to separately plot each relationship. An example of such plot is shown in figure \ref{fig:time_feature_ex1}. 

\begin{figure}[!hb]
	\centering
	\input{Figures/time_feature_ex1.tex}
	\caption{Plot of \texttt{Time} and \texttt{Texture Mean} }
	\label{fig:time_feature_ex1}
\end{figure}

Unfortunately, most plots show very scattered points that do not seem to follow any specific model. The variance is so significant that we cannot even estimate the type of function that links a feature and the variable \texttt{Time} together. It could be a simple linear model with a high variance that we may be able to estimate, or more complex models with non-linearities and feature correlations.

To have better clues on the type of model we should be dealing with, we can draw a QQ-Plot that plots the Studentized Residuals with the quantiles. 

In R, we can use the library \texttt{car} to easily draw the QQ-Plot (figure \ref{fig:qq_plot}) :
\begin{lstlisting}
library(car)
qqPlot(linreg, main="QQ Plot")
\end{lstlisting}

\begin{figure}[!hb]
	\centering
	\input{Figures/qq_plot.tex}
	\caption{QQ-Plot}
	\label{fig:qq_plot}
\end{figure}

The bottom tail of the QQ-Plot seems to deviate from the linear line, which is a sign of the error's non-normality. This may mean that the error does not follow a normal model, or that the model is actually non-linear.

\begin{figure}[!hb]
	\centering
	\includegraphics{Figures/fitted_value_plots}
	\caption{}
	\label{fig:fitted_value_plots}
\end{figure}

We can also analyze the Residuals-Fitted plot and the Scale-Location plot to get a better understanding on the model. To do, we can simply apply the function \texttt{plot} on the linear model (figure \ref{fig:fitted_value_plots}).

It turns out that both plots show non-normal scatters of the residuals. In particular, the points displayed in the Residuals-Fitted plot seem to follow a fan shape. This is a sign of a non-constant variance, also called heteroscedasticity.

\section{Measures to Compare Models}
Before building any model, we have to properly define the measures we will later use to compare them. 

\subsection{Some Measures}
A first way to assess the performance of a model is to compute the Mean Squared Error (MSE). We can also use adjusted $R^2$ score.

\subsection{Data Split}
These measures should not be applied on a set whose data was also used to train the model. Indeed, this would include a biais that might distort our conclusions. To cope with this problem, we have to split the dataset into two disjointed sets : 
\begin{itemize}
	\item Training Set : About 75\% of the dataset dedicated to the building the model;
	\item Test Set : The remaining 25\% only used at the end to provide some kind of objective measure of the model performance.
\end{itemize} 

Once it is done, we can finally dive in the model building.

\section{K-nearest neighbors (KNN)}
\subsection{Idea}
We start our analysis with a very simple model called the KNN.
Given an positive integer k and a test observation x0. The KNN model first identifies the k closest points to x0 from the training data. Then estimates \\ 

The KNN model in R is done by calling the function reg of the package knn. As we will see in the following sections, For most prediction algorithms, we first have to build the prediction model on the training data, and then use the model to test our predictions. However, the KNN function does both in a single step.\\ 
In order to find the best k we set a maximum number of neighbors to be considered (in our model it is 20), then we calculate the MSE for each k which is the mean of the squared difference between the real value of Time and the predicted one. All the steps are detailed in the code below.\\

\begin{lstlisting}
library(FNN)
library(tikzDevice)
k_max = 120;
MSE = rep(0,k_max)

for( k in 1:k_max)
{
reg = knn.reg(train=cancer.train.x, test=cancer.test.x, y=cancer.train.y, k=k)
MSE[k] = mean((cancer.test.y - reg$pred)^2)
}

best_k_test = which.min(MSE)
best_k_mse = MSE[best_k_test]
sprintf("Best knn1 = %d and the best MSE1 = %f", best_k_test, best_k_mse)

tikz('Figures/knn.tex',width=5, height=5)

plot(1:k_max, MSE, xlab='k', ylab='MSE', main='MSE against k neighbours')
points(x = best_k_test, y = best_k_mse, col = "red", pch = 16)
abline(h = best_k_mse, col='red')
abline(v = best_k_test, col='red')
dev.off()
\end{lstlisting}

The graph below shows the MSE plotted against the values of k in a range from 1 to 20. We can notice that a minimum is reached between 10 and 20. We use the function which.min that returns the index of the minimum value.\\
	
\begin{figure}[!hb]
	\centering
	\input{Figures/knn.tex}
	\caption{MSE against k neighbours}
\end{figure}

 The minimum MSE which yields to the best k is colored in red. Its coordinates correspond to (\textbf{12,967.386966}). The best k is therefore .
Now that we have the k that minimizes the MSE we call KNN algorithm with this best k and plot the predicted values against the real values. The figure above shows the result. The red line is the function y=x; so further are the points from this line the further are the predicted values ($\hat{y}$) from the real one (y).

\begin{lstlisting}
best_reg_test = knn.reg(train= cancer.train.x, test = cancer.test.x, y=cancer.train.y, k = best_k_test)
tikz('Figures/knn_predicted_test.tex',width=5, height=5)
plot(cancer.test.y, best_reg_test$pred, xlab='y', ylab='y-hat', main='y-hat (Predicted) against y')
abline(0,1, col='red')
dev.off() 
\end{lstlisting}

\begin{figure}[!h]
	\centering
	\input{Figures/knn_predicted_test.tex}
	\caption{$\hat{y}$ against y}
\end{figure}

When notice that the predicted {$\hat{y}$} diverge a lot the real values y. We actually expected those results since the MSE=967.386966 which is quite high. 


\subsection{Best $k$: CROSS VALIDATION}
In our previous reasoning was quite optimistic because we tried finding the best k with minimizing the MSE in the test data. Therefore the model is very specific to our test data which yield to a high bias. The solution here is to find the best k among the train data and then use the best k in the test data. \\ 
To find the best k number of neighbors we use the method of cross validation  on the train data. There are two methods in cross validation: \textbf{cross validation leave one out} and  \textbf{K-fold cross validation}. As we do not have that much predictors we can afford the computation of cross validation leave one out.\\

\begin{lstlisting}
library("kknn")
model_kknn = train.kknn(Time ~., data= cancer.train, kmax = 30, ks = NULL, distance = 2, kernel = "optimal")
best_k_train = model_kknn$best.parameters$k
\end{lstlisting}


\begin{lstlisting}
library("kknn")
model_kknn = train.kknn(Time ~., data= cancer.train, kmax = 30, ks = NULL, distance = 2, kernel = "optimal")
best_k_train = model_kknn$best.parameters$k
\end{lstlisting}


On the test data with the best training k form the LOOCV model	
	
\begin{lstlisting}
best_reg_train = knn.reg(train= cancer.train.x, test = cancer.test.x, y=cancer.train.y, k = best_k_train)
tikz('/Users/slam/Desktop/Git/SY19-TP1/Figures/knn_predicted_LOOCV.tex',width=5, height=5)
plot(cancer.test.y, best_reg_train$pred, xlab='y', ylab='prediction')
abline(0,1, col='red')
dev.off() 
\end{lstlisting}

\begin{figure}[!h]
	\centering
	\input{Figures/knn_predicted_LOOCV.tex}
	\caption{$\hat{y}$ against y}
\end{figure}


The prediction is not better but the model is not biased

\section{Simple Linear Regression}
\subsection{Idea}
Our next attempt consists in using the same linear model we used in the feature analysis section. This model takes advantage of the simple assumption that \texttt{Time} linearily depends on the other features.

To build the model, we can use the function \texttt{lm} :
\begin{lstlisting}
model.linreg = lm(Time ~ ., data=train_set)
\end{lstlisting}


\subsection{Model Performance}
The MSE of this model is approximatively equal to 1285. The raw residual distribution is shown on figure \ref{fig:linreg_hist}. 

\begin{figure}[!h]
	\centering
	\input{Figures/linreg_hist.tex}
	\caption{}
	\label{fig:linreg_hist}
\end{figure}

\section{Linear Regression with Features Selection}
\subsection{Idea}
A simple method to improve the performance of a simple Linear Regression is to select a subset of features that better describes the distribution of \texttt{Time}.

Once the simple linear model is fitted, we can use the function \texttt{summary} to display the value of each coefficient.
\begin{verbatim}
Coefficients:
Estimate Std. Error t value Pr(>|t|)  
(Intercept)              5.722e+01  1.587e+02   0.361   0.7191  
Lymph_node              -1.752e-01  7.034e-01  -0.249   0.8038  
radius_mean              3.001e+01  4.445e+01   0.675   0.5012  
texture_mean            -3.913e-01  2.043e+00  -0.192   0.8485  
perimeter_mean          -3.794e+00  6.651e+00  -0.570   0.5697  
area_mean               -1.328e-01  1.354e-01  -0.981   0.3291  
smoothness_mean         -7.124e+02  9.050e+02  -0.787   0.4331  
compactness_mean        -8.767e+01  3.554e+02  -0.247   0.8057  
concavity_mean          -2.654e+02  2.728e+02  -0.973   0.3332  
concave_points_mean      1.236e+03  5.911e+02   2.090   0.0392 *
symmetry_mean           -3.199e+01  2.647e+02  -0.121   0.9041  
fractal_dimension_mean   1.818e+03  1.666e+03   1.091   0.2781  
radius_se               -2.897e+01  9.953e+01  -0.291   0.7716  
texture_se              -1.784e+00  1.246e+01  -0.143   0.8865  
perimeter_se             8.817e+00  1.289e+01   0.684   0.4955  
area_se                 -3.871e-01  4.212e-01  -0.919   0.3604  
smoothness_se            2.987e+03  2.535e+03   1.178   0.2415  
compactness_se           7.135e+02  7.864e+02   0.907   0.3665  
concavity_se             3.798e+02  7.170e+02   0.530   0.5975  
concave_points_se       -7.782e+02  1.450e+03  -0.537   0.5926  
symmetry_se             -1.239e+03  7.861e+02  -1.576   0.1182  
fractal_dimension_se    -5.979e+03  6.280e+03  -0.952   0.3434  
radius_worst             9.300e-01  1.367e+01   0.068   0.9459  
texture_worst           -1.307e+00  1.964e+00  -0.666   0.5072  
perimeter_worst         -1.026e+00  1.449e+00  -0.708   0.4805  
area_worst               8.705e-02  7.021e-02   1.240   0.2181  
smoothness_worst        -3.761e+02  4.088e+02  -0.920   0.3599  
compactness_worst       -7.031e+01  9.964e+01  -0.706   0.4821  
concavity_worst         -3.011e+01  7.311e+01  -0.412   0.6814  
concave_points_worst    -2.181e+02  2.345e+02  -0.930   0.3546  
symmetry_worst           1.586e+02  1.425e+02   1.113   0.2687  
fractal_dimension_worst  1.028e+03  7.100e+02   1.448   0.1509  
Tumor_size              -1.188e+00  1.915e+00  -0.620   0.5364  
\end{verbatim}

The last column contains the P-value of each coefficient, which is a measure to test the hypothesis that this particular coefficient is null. A P-value lower than 5\% allows us to conclude that the coefficient is not equal to zero. In our case, the feature \texttt{concavity\_points\_mean} is not null, but we cannot make such assumptions for the other parameters. Therefore, we are not able to select a subset of interesting features based on the P-values.

Our dataset contains a small set of features to deal with, therefore we can use an exhaustive feature subset selection algorithm. Such method build a linear model based on each subsets and compute a performance score to compare them. 

We use the following function to fit the linear models :
\begin{lstlisting}
model.linreg.regsubsets = regsubsets(Time ~ ., data=train_set, method = "exhaustive", nvmax = 32)
\end{lstlisting}

Then the function \texttt{plot} to compare the models according to a given scale (in this case, the BIC measure). The result is shown on figure \ref{fig:subset_bic}.
\begin{lstlisting}
plot(model.linreg.regsubsets, scale="bic")	
\end{lstlisting}

\begin{figure}[!h]
	\centering
	\input{Figures/subset_bic.tex}
	\caption{}
	\label{fig:subset_bic}
\end{figure}

According to this plot, the best BIC is reached with a model that only uses the following features : 
\begin{itemize}
	\item \texttt{texture\_mean}
	\item \texttt{fractal\_dimension\_mean}
	\item \texttt{concavity\_mean}
\end{itemize}

\subsection{Model Performance}
The MSE of this model is approximatively equal to 1067. The raw residual distribution is shown on figure \ref{fig:subset_hist}. 

\begin{figure}[!h]
	\centering
	\input{Figures/subset_hist.tex}
	\caption{}
	\label{fig:subset_hist}
\end{figure}

\section{Linear Regression with Regularization}

*RIDGE + LASSO*
\subsection{Idea}

\subsection{Build the Model}

\subsection{Model Analysis}



\section{Models Comparaison}

*USE TEST SET TO COMPARE MODEL*

\end{document}
