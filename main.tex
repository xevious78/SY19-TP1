\documentclass[]{report}
\usepackage{tikz}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%opening
\title{SY19}
\author{}

\begin{document}
	
\lstset{frame=tb,
	language=R,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

\maketitle

\part{Abreviations}
	

\begin{abstract}

\end{abstract}

\part{Introduction}

\part{Ex 1}

\part{Ex 2 - Breast Cancer Recurring Time}

\section{Introduction}
This part aims to build the best model to predict the recurring time of breast cancer based on about 30 features computed from a breast mass.  This regression problem will take advantage of a given dataset describing about 200 patient cases.

\section{Dataset Description}
We take a look at the raw dataset to get first hints on how each feature contributes to the recurring time.

The dataset comprises 194 patient cases, each of which is described through 32 features and the cancer recurring time \texttt{Time} that we have to predict.

\subsection{Time}
Let's first describe the evolution of the variable \texttt{Time}. To do so, we can use the R functions \texttt{boxplot} and \texttt{hist}.

\begin{figure}[!hb]
	\centering
	\input{Figures/time_boxplot.tex}
	\caption{Box Plot}
\end{figure}

\begin{figure}[!hb]
	\centering
	\input{Figures/time_hist.tex}
	\caption{Histogram}
\end{figure}

\subsection{Features Description}
Each patient is represented with a set of 32 features extracted and computed from a digitized image of a breast mass. 

The data description we were given does not specify the units, but we do not need them for the following analysis.

Here are the 32 features we are provided with:  
\begin{itemize}
	\item Lymph Node Status
	
	\item Mean of 
		\begin{itemize}
			\item radius
			\item texture
			\item perimeter
			\item area
			\item smoothness
			\item compactness
			\item concavity
			\item concave points
			\item symmetry
			\item fractal dimension
		\end{itemize}
	
	\item Standard Error of 
		\begin{itemize}
		\item radius
		\item texture
		\item perimeter
		\item area
		\item smoothness
		\item compactness
		\item concavity
		\item concave points
		\item symmetry
		\item fractal dimension
	\end{itemize}

	\item Mean of the three largest values (also called "Worst") of 
		\begin{itemize}
		\item radius
		\item texture
		\item perimeter
		\item area
		\item smoothness
		\item compactness
		\item concavity
		\item concave points
		\item symmetry
		\item fractal dimension
	\end{itemize}

	\item Tumor Size
\end{itemize}

\subsubsection{Feature Correlation}
Based on the definition of the parameters described above, we already know that many features are correlated :
\begin{itemize}
	\item The mean of each parameters is smaller than the "worst" value;
	\item The radius, the perimeter and the area might be linked together;
	\item The compactness can be computed with the perimeter and the area thanks to the given formula : $Compactness = \frac{perimeter^2}{area - 1}$
\end{itemize}

\subsection{Data Relevance}
We should first check that every patient is relevant to our study, in other words, that there is no abnormal observation in the dataset. Cook's Distance is an interesting measure to verify this important criteria, it can be computed after a simple Linear Regression.

Cook's distance aims to study the influence of each observation on the regression coefficient estimates. To do so, this method uses a straight-forward approach that consists in computing the difference between the original coefficient estimates $\hat{\beta}$ and the coefficient estimates without taking into account the $i$th observation $\hat{\beta}_{-i}$. The difference is then normalized using the number of parameters and the standard deviation estimate. 

A value higher than 1 often indicates an outlier that should be removed from the dataset.

In R, we can use the following code to compute and plot the Cook's distance of each observation :

\begin{lstlisting}
linreg = lm(Time ~ ., data=data_set)
cooks.distance(linreg)
\end{lstlisting} 

\begin{figure}[!hb]
	\centering
	\input{Figures/cooks_distance.tex}
	\caption{Cook's Distance}
	\label{fig:cook_distance}
\end{figure}

According to plot (figure \ref{fig:cook_distance}), no observation is located beyond the critical Cook's boundary of 1. This means that we can potentially use each and every patient case of our dataset to build our regression model. 

\subsection{Relation between "feature" and "time"}
In this section, we will take a first look at the relationship between the variable \texttt{Time} and the features used to describe a patient case. 

A first way to do it is to separately plot each relationship. An example of such plot is shown in figure \ref{fig:time_feature_ex1}. 

\begin{figure}[!hb]
	\centering
	\input{Figures/time_feature_ex1.tex}
	\caption{Plot of \texttt{Time} and \texttt{Texture Mean} }
	\label{fig:time_feature_ex1}
\end{figure}

Unfortunately, most plots show very scattered points that do not seem to follow any specific model. The variance is so significant that we cannot even estimate the type of function that links a feature and the variable \texttt{Time} together. It could be a simple linear model with a high variance that we may be able to estimate, or more complex models with non-linearities and feature correlations.

To have better clues on the type of model we should be dealing with, we can draw a QQ-Plot that plots the Studentized Residuals with the quantiles. 

In R, we can use the library \texttt{car} to easily draw the QQ-Plot (figure \ref{fig:qq_plot}) :
\begin{lstlisting}
library(car)
qqPlot(linreg, main="QQ Plot")
\end{lstlisting}

\begin{figure}[!hb]
	\centering
	\input{Figures/qq_plot.tex}
	\caption{QQ-Plot}
	\label{fig:qq_plot}
\end{figure}

The bottom tail of the QQ-Plot seems to deviate from the linear line, which is a sign of the error's non-normality. This may mean that the error does not follow a normal model, or that the model is actually non-linear.

\begin{figure}[!hb]
	\centering
	\includegraphics{Figures/fitted_value_plots}
	\caption{}
	\label{fig:fitted_value_plots}
\end{figure}

We can also analyze the Residuals-Fitted plot and the Scale-Location plot to get a better understanding on the model. To do, we can simply apply the function \texttt{plot} on the linear model (figure \ref{fig:fitted_value_plots}).

It turns out that both plots show non-normal scatters of the residuals. In particular, the points displayed in the Residuals-Fitted plot seem to follow a fan shape. This is a sign of a non-constant variance, also called heteroscedasticity.

\section{Measures to Compare Models}
Before building any model, we have to properly define the measures we will later use to compare them. 

\subsection{Some Measures}
A first way to assess the performance of a model is to compute the Mean Squared Error (MSE). We can also use adjusted $R^2$ score.

\subsection{Data Split}
These measures should not be applied on a set whose data was also used to train the model. Indeed, this would include a biais that might distort our conclusions. To cope with this problem, we have to split the dataset into two disjointed sets : 
\begin{itemize}
	\item Training Set : About 75\% of the dataset dedicated to the building the model;
	\item Test Set : The remaining 25\% only used at the end to provide some kind of objective measure of the model performance.
\end{itemize} 

Once it is done, we can finally dive in the model building.

\section{K-nearest neighbors (KNN)}
\subsection{Idea}
We start our analysis with a very simple model called the KNN.
Given an positive integer k and a test observation x0. The KNN model first identifies the k closest points to x0 from the training data. Then estimates \\ 

The KNN model in R is done by calling the function reg of the package knn. As we will see in the following sections, For most prediction algorithms, we first have to build the prediction model on the training data, and then use the model to test our predictions. However, the KNN function does both in a single step.\\ 
In order to find the best k we set a maximum number of neighbors to be considered (in our model it is 20), then we calculate the MSE for each k which is the mean of the squared difference between the real value of Time and the predicted one. All the steps are detailed in the code below.\\

\begin{lstlisting}
library(FNN)
library(tikzDevice)
k_max = 120;
MSE = rep(0,k_max)

for( k in 1:k_max)
{
reg = knn.reg(train=cancer.train.x, test=cancer.test.x, y=cancer.train.y, k=k)
MSE[k] = mean((cancer.test.y - reg$pred)^2)
}

best_k_test = which.min(MSE)
best_k_mse = MSE[best_k_test]
sprintf("Best knn1 = %d and the best MSE1 = %f", best_k_test, best_k_mse)

tikz('Figures/knn.tex',width=5, height=5)

plot(1:k_max, MSE, xlab='k', ylab='MSE', main='MSE against k neighbours')
points(x = best_k_test, y = best_k_mse, col = "red", pch = 16)
abline(h = best_k_mse, col='red')
abline(v = best_k_test, col='red')
dev.off()
\end{lstlisting}

The graph below shows the MSE plotted against the values of k in a range from 1 to 20. We can notice that a minimum is reached between 10 and 20. We use the function which.min that returns the index of the minimum value.\\
	
\begin{figure}[!hb]
	\centering
	\input{Figures/knn.tex}
	\caption{MSE against k neighbours}
\end{figure}

 The minimum MSE which yields to the best k is colored in red. Its coordinates correspond to (\textbf{12,967.386966}). The best k is therefore .
Now that we have the k that minimizes the MSE we call KNN algorithm with this best k and plot the predicted values against the real values. The figure above shows the result. The red line is the function y=x; so further are the points from this line the further are the predicted values ($\hat{y}$) from the real one (y).

\begin{lstlisting}
best_reg_test = knn.reg(train= cancer.train.x, test = cancer.test.x, y=cancer.train.y, k = best_k_test)
tikz('Figures/knn_predicted_test.tex',width=5, height=5)
plot(cancer.test.y, best_reg_test$pred, xlab='y', ylab='y-hat', main='y-hat (Predicted) against y')
abline(0,1, col='red')
dev.off() 
\end{lstlisting}

\begin{figure}[!h]
	\centering
	\input{Figures/knn_predicted_test.tex}
	\caption{$\hat{y}$ against y}
\end{figure}

When notice that the predicted {$\hat{y}$} diverge a lot the real values y. We actually expected those results since the MSE=967.386966 which is quite high. 


\subsection{Best $k$: CROSS VALIDATION}
In our previous reasoning was quite optimistic because we tried finding the best k with minimizing the MSE in the test data. Therefore the model is very specific to our test data which yield to a high bias. The solution here is to find the best k among the train data and then use the best k in the test data. \\ 
To find the best k number of neighbors we use the method of cross validation  on the train data. There are two methods in cross validation: \textbf{cross validation leave one out} and  \textbf{K-fold cross validation}. As we do not have that much predictors we can afford the computation of cross validation leave one out.\\

\begin{lstlisting}
library("kknn")
model_kknn = train.kknn(Time ~., data= cancer.train, kmax = 30, ks = NULL, distance = 2, kernel = "optimal")
best_k_train = model_kknn$best.parameters$k
\end{lstlisting}


\begin{lstlisting}
library("kknn")
model_kknn = train.kknn(Time ~., data= cancer.train, kmax = 30, ks = NULL, distance = 2, kernel = "optimal")
best_k_train = model_kknn$best.parameters$k
\end{lstlisting}


On the test data with the best training k form the LOOCV model	
	
\begin{lstlisting}
best_reg_train = knn.reg(train= cancer.train.x, test = cancer.test.x, y=cancer.train.y, k = best_k_train)
tikz('/Users/slam/Desktop/Git/SY19-TP1/Figures/knn_predicted_LOOCV.tex',width=5, height=5)
plot(cancer.test.y, best_reg_train$pred, xlab='y', ylab='prediction')
abline(0,1, col='red')
dev.off() 
\end{lstlisting}

\begin{figure}[!h]
	\centering
	\input{Figures/knn_predicted_LOOCV.tex}
	\caption{$\hat{y}$ against y}
\end{figure}


The prediction is not better but the model is not biased

\section{Simple Linear Regression}
\subsection{Idea}

\subsection{Build the Model} 

\subsection{Model Analysis}
*HIGH P VALUES*

\section{Linear Regression with Features Selection}
\subsection{Idea}

\subsection{Build the Model}
*EXHAUSTIVE*
\subsection{Model Analysis}

\section{Linear Regression with Regularization}

*RIDGE + LASSO*
\subsection{Idea}

\subsection{Build the Model}

\subsection{Model Analysis}



\section{Models Comparaison}
*USE TEST SET TO COMPARE MODEL*

\end{document}
